{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHGZbmvoJGVj"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import os\n",
        "from keras.callbacks import EarlyStopping\n",
        "from datasets import load_dataset, Dataset\n",
        "import csv\n",
        "import re\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.layers  import Layer\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import constraints\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "#from keras.layers.recurrent import LSTM, GRU,SimpleRNN\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, Embedding\n",
        "\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence, text\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from plotly import graph_objs as go\n",
        "from keras.layers import Dense, Activation, Embedding, Input\n",
        "from keras.models import Model"
      ],
      "metadata": {
        "id": "3ikWTlcuKIUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hyperparameters"
      ],
      "metadata": {
        "id": "hwJZNjcnYQT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "batch_size = 32\n",
        "margin = 1  # Margin for constrastive loss.\n",
        "max_len = 1500"
      ],
      "metadata": {
        "id": "VM-wWvBSYUfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "8Io_WW5LKZ_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading banking77 dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"PolyAI/banking77\" )"
      ],
      "metadata": {
        "id": "KzKBirFtKcyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "1z1TbSw1LCcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset['train']['text']\n",
        "y = dataset['train']['label']\n",
        "\n",
        "data_test = dataset['test']['text']\n",
        "y_test = dataset['test']['label']\n",
        "\n",
        "\n",
        "#convert list to pandas series\n",
        "data = pd.Series(data)\n",
        "y  = pd.Series(y)\n",
        "\n",
        "data_test = pd.Series(data_test)\n",
        "y_test  = pd.Series(y_test)"
      ],
      "metadata": {
        "id": "57wiLbNcLGgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting train dataset to training and validation parts\n",
        "\n",
        "xtrain, xvalid, ytrain, yvalid = train_test_split(data, y,\n",
        "                                                  stratify=y,\n",
        "                                                  random_state=42,\n",
        "                                                  test_size=0.1, shuffle=True)\n"
      ],
      "metadata": {
        "id": "k--xdeIkLM9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using keras tokenizer here\n",
        "token = text.Tokenizer(num_words=None)\n",
        "max_len = 1500\n",
        "\n",
        "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
        "xtrain_seq = token.texts_to_sequences(xtrain)\n",
        "xvalid_seq = token.texts_to_sequences(xvalid)\n",
        "#xtest_seq  = token.texts_to_sequences(data_test)\n",
        "\n",
        "#zero pad the sequences\n",
        "xtrain_pad = pad_sequences(xtrain_seq, maxlen=max_len)\n",
        "xvalid_pad = pad_sequences(xvalid_seq, maxlen=max_len)\n",
        "#xtest_pad  = pad_sequences(xtest_seq, maxlen=max_len)\n",
        "\n",
        "word_index = token.word_index"
      ],
      "metadata": {
        "id": "DJm7PePuLvgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "D9J2bxjqLw7H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocmzYjijt7j-"
      },
      "outputs": [],
      "source": [
        "# Aspect modeling based on orginal paper \"An Unsupervised Neural Attention Model for Aspect Extraction\"\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self, W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "        \"\"\"\n",
        "        Keras Layer that implements an Content Attention mechanism.\n",
        "        Supports Masking.\n",
        "        \"\"\"\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.glorot_uniform\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert type(input_shape) == list\n",
        "        assert len(input_shape) == 2\n",
        "\n",
        "        self.steps = input_shape[0][1]\n",
        "\n",
        "        self.W = self.add_weight(shape=(input_shape[0][-1], input_shape[1][-1]),\n",
        "                                    initializer=self.init,\n",
        "                                    name='{}_W'.format(self.name),\n",
        "                                    regularizer=self.W_regularizer,\n",
        "                                    constraint=self.W_constraint)\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(shape=(1,),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input_tensor, mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, input_tensor, mask=None):\n",
        "        x = input_tensor[0]\n",
        "        y = input_tensor[1]\n",
        "        #mask = mask[0]\n",
        "\n",
        "        y = K.transpose(K.dot(self.W, K.transpose(y)))\n",
        "        y = K.expand_dims(y, -2)\n",
        "        y = K.repeat_elements(y, self.steps, axis=1)\n",
        "        eij = K.sum(x*y, axis=-1)\n",
        "\n",
        "        if self.bias:\n",
        "            b = K.repeat_elements(self.b, self.steps, axis=0)\n",
        "            eij += b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        if mask is not None:\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "        return a\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0][0], input_shape[0][1])\n",
        "\n",
        "class WeightedSum(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.supports_masking = True\n",
        "        super(WeightedSum, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, input_tensor, mask=None):\n",
        "        assert type(input_tensor) == list\n",
        "        #assert type(mask) == list   #this line has been changed by me\n",
        "\n",
        "        x = input_tensor[0]\n",
        "        a = input_tensor[1]\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0][0], input_shape[0][-1])\n",
        "\n",
        "    def compute_mask(self, x, mask=None):\n",
        "        return None\n",
        "\n",
        "class WeightedAspectEmb(Layer):\n",
        "    def __init__(self, input_dim, output_dim,\n",
        "                 init='uniform', input_length=None,\n",
        "                 W_regularizer=None, activity_regularizer=None,\n",
        "                 W_constraint=None,\n",
        "                 weights=None, dropout=0., **kwargs):\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.init = initializers.get(init)\n",
        "        self.input_length = input_length\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "\n",
        "        if 0. < self.dropout < 1.:\n",
        "            self.uses_learning_phase = True\n",
        "        self.initial_weights = weights\n",
        "        kwargs['input_shape'] = (self.input_length,)\n",
        "        dtype = K.floatx()\n",
        "        super(WeightedAspectEmb, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "\n",
        "        if self.initial_weights is not None:\n",
        "            self.set_weights(self.initial_weights)\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, x, mask=None):\n",
        "        return None\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.output_dim)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        return K.dot(x, self.W)\n",
        "\n",
        "\n",
        "class Average(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.supports_masking = True\n",
        "        super(Average, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = K.cast(mask, K.floatx())\n",
        "            mask = K.expand_dims(mask)\n",
        "            x = x * mask\n",
        "        return K.sum(x, axis=-2) / K.sum(mask, axis=-2)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0:-2]+input_shape[-1:]\n",
        "\n",
        "    def compute_mask(self, x, mask=None):\n",
        "        return None\n",
        "\n",
        "\n",
        "class MaxMargin(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MaxMargin, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, input_tensor, mask=None):\n",
        "        z_s = input_tensor[0]\n",
        "        z_n = input_tensor[1]\n",
        "        r_s = input_tensor[2]\n",
        "\n",
        "        z_s = z_s / K.cast(K.epsilon() + K.sqrt(K.sum(K.square(z_s), axis=-1, keepdims=True)), K.floatx())\n",
        "        z_n = z_n / K.cast(K.epsilon() + K.sqrt(K.sum(K.square(z_n), axis=-1, keepdims=True)), K.floatx())\n",
        "        r_s = r_s / K.cast(K.epsilon() + K.sqrt(K.sum(K.square(r_s), axis=-1, keepdims=True)), K.floatx())\n",
        "\n",
        "        steps = z_n.shape[1]\n",
        "\n",
        "        pos = K.sum(z_s*r_s, axis=-1, keepdims=True)\n",
        "\n",
        "        pos = K.repeat_elements(pos, steps, axis=-1)\n",
        "        r_s = K.expand_dims(r_s, -2)\n",
        "        r_s = K.repeat_elements(r_s, steps, axis=1)\n",
        "        neg = K.sum(z_n*r_s, axis=-1)\n",
        "\n",
        "        loss = K.cast(K.sum(K.maximum(0., (1. - (pos + neg))), axis=-1, keepdims=True), K.floatx())\n",
        "        return loss\n",
        "\n",
        "    def compute_mask(self, input_tensor, mask=None):\n",
        "        return None\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0][0], 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4qtngp_wR4_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aspect model with softmax head"
      ],
      "metadata": {
        "id": "4u-m4Xo0UTuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the model with keras\n",
        "\n",
        "input = layers.Input(shape=(1500,), dtype='int32')\n",
        "word_emb = Embedding(len(word_index)  , 300 ,trainable=True, mask_zero=True, name='word_emb')\n",
        "e_w = word_emb(input)\n",
        "\n",
        "y_s = Average()(e_w)\n",
        "\n",
        "att_weights = Attention(name='att_weights')([e_w, y_s])\n",
        "z_s = WeightedSum()([e_w, att_weights])\n",
        "\n",
        "output = layers.Dense(77)(z_s)\n",
        "\n",
        "cls_model = keras.Model(inputs= input, outputs=output) # creating the model with out 0 or 1 (binary output)\n",
        "\n",
        "# compiling the model\n",
        "cls_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=\"adam\", metrics = ['accuracy'])\n",
        "cls_model.summary()\n",
        "\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 5)\n",
        "history = cls_model.fit(\n",
        "    xtrain_pad,\n",
        "    ytrain,\n",
        "    validation_data=(xvalid_pad, yvalid),\n",
        "    batch_size=8,\n",
        "    epochs=20,\n",
        "    callbacks=[es]\n",
        ")\n",
        "\n",
        "# evaluate the model on the test dataset\n",
        "\n",
        "cls_model.evaluate(xtest_pad, y_test, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "mVhBKkwHUZqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aspect model with TF-idf"
      ],
      "metadata": {
        "id": "d_Qgd-shVPSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "itlCq_MjVT79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "\n",
        "# building tf-idf vectors for each sample in our dataset\n",
        "tfv = TfidfVectorizer(min_df=1,  max_features=None,\n",
        "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
        "            ngram_range=(1, 1), use_idf=True,smooth_idf=1,sublinear_tf=1,\n",
        "            #stop_words = 'english'\n",
        ")\n",
        "\n",
        "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
        "tfv.fit(list(xtrain) + list(xvalid))\n",
        "xtrain_tfv =  tfv.transform(xtrain)\n",
        "xvalid_tfv = tfv.transform(xvalid)\n",
        "xtest_tfv = tfv.transform(data_test)\n",
        "\n",
        "\n",
        "xtrain_tfv = xtrain_tfv.toarray()\n",
        "xvalid_tfv = xvalid_tfv.toarray()\n",
        "xtest_tfv = xtest_tfv.toarray()\n"
      ],
      "metadata": {
        "id": "GClW-4qWVYg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the model with keras\n",
        "\n",
        "input1 = layers.Input(shape=(1500,), dtype='int32')\n",
        "input2 = layers.Input(shape=(1129,), dtype='float32')\n",
        "word_emb = Embedding(len(word_index) , 300,trainable=True, mask_zero=True, name='word_emb')\n",
        "e_w = word_emb(input1)\n",
        "y_s = Average()(e_w)\n",
        "att_weights = Attention(name='att_weights')([e_w, y_s])\n",
        "z_s = WeightedSum()([e_w, att_weights])\n",
        "z_s = tf.concat(axis=1,values=[z_s, input2])\n",
        "\n",
        "output = layers.Dense(77, activation='softmax')(z_s)\n",
        "cls_model = keras.Model(inputs= [input1, input2], outputs=output)\n",
        "\n",
        "\n",
        "#compile and fit the model\n",
        "cls_model.compile(loss='SparseCategoricalCrossentropy', optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "cls_model.summary()\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 5)\n",
        "history = cls_model.fit(\n",
        "    [xtrain_pad, xtrain_tfv],\n",
        "    ytrain,\n",
        "    validation_data=([xvalid_pad, xvalid_tfv], yvalid),\n",
        "    batch_size=8,\n",
        "    epochs=20,\n",
        "    callbacks=[es]\n",
        ")\n"
      ],
      "metadata": {
        "id": "SD-BHqfxVt9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate the model on the test dataset\n",
        "cls_model.evaluate([xtest_pad, xtest_tfv], y_test, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "cPEErYVBWLai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aspect with Bert"
      ],
      "metadata": {
        "id": "n_-JEKRYWilC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization the input for the bert model\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "train_tokenized = tokenizer(list(xtrain),padding='max_length', truncation=True, max_length=128)\n",
        "valid_tokenized = tokenizer(list(xvalid),padding='max_length', truncation=True, max_length=128)\n",
        "test_tokenized = tokenizer(list(data_test),padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "train_input_ids = np.array(train_tokenized['input_ids'])\n",
        "valid_input_ids = np.array(valid_tokenized['input_ids'])\n",
        "test_input_ids = np.array(test_tokenized['input_ids'])\n",
        "\n",
        "\n",
        "\n",
        "train_mask = np.array(train_tokenized['attention_mask'])\n",
        "valid_mask = np.array(valid_tokenized['attention_mask'])\n",
        "test_mask = np.array(test_tokenized['attention_mask'])"
      ],
      "metadata": {
        "id": "gTIwGGxsWmkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the bert model\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "import tensorflow as tf\n",
        "\n",
        "#tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states = True)"
      ],
      "metadata": {
        "id": "YjeIDyPhW5uM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the model with keras\n",
        "\n",
        "\n",
        "input1 = layers.Input(shape=(128,), dtype='int32')\n",
        "\n",
        "\n",
        "#attention_mask = K.not_equal(input1,  0 )\n",
        "mask = K.not_equal(input1,  0 or 101 or 102 )\n",
        "attention_mask = K.not_equal(input1,  0  )\n",
        "\n",
        "hidden_states = bert_model(input1, attention_mask = attention_mask)\n",
        "cls = hidden_states[1]\n",
        "e_w = hidden_states[0][:,1:,:]\n",
        "\n",
        "\n",
        "#y_s = Average()(e_w)\n",
        "att_weights = Attention(name='att_weights')([e_w, cls] )\n",
        "z_s = WeightedSum()([e_w, att_weights],  )\n",
        "z_s = tf.concat(axis=1,values=[z_s, cls])\n",
        "output = layers.Dense(77, activation=\"softmax\")(z_s)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cls_model = keras.Model(inputs=input1, outputs=output) # creating the model with out 0 or 1 (binary output)\n",
        "\n",
        "#compile and fit the model\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 3)\n",
        "cls_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
        ")\n",
        "history = cls_model.fit(\n",
        "    [train_input_ids],\n",
        "    ytrain,\n",
        "    validation_data=(valid_input_ids, yvalid),\n",
        "    batch_size=32,\n",
        "    epochs=5,\n",
        "    callbacks = [es]\n",
        ")"
      ],
      "metadata": {
        "id": "OpNZjjpiXbkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate the model on the test datset\n",
        "print(\"Evaluate on test data\")\n",
        "results = cls_model.evaluate([valid_input_ids, valid_mask], yvalid, batch_size=32)\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "metadata": {
        "id": "uAtPWAncX93W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bert with TF-idf\n"
      ],
      "metadata": {
        "id": "_vrWpZE2gCdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#input tokenization for bert model\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "train_tokenized = tokenizer(list(xtrain),padding='max_length', truncation=True, max_length=128)\n",
        "valid_tokenized = tokenizer(list(xvalid),padding='max_length', truncation=True, max_length=128)\n",
        "test_tokenized = tokenizer(list(data_test),padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "train_input_ids = np.array(train_tokenized['input_ids'])\n",
        "valid_input_ids = np.array(valid_tokenized['input_ids'])\n",
        "test_input_ids = np.array(test_tokenized['input_ids'])\n",
        "\n",
        "\n",
        "\n",
        "train_mask = np.array(train_tokenized['attention_mask'])\n",
        "valid_mask = np.array(valid_tokenized['attention_mask'])\n",
        "test_mask = np.array(test_tokenized['attention_mask'])"
      ],
      "metadata": {
        "id": "Fn6M186kgGB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading bert model\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "import tensorflow as tf\n",
        "\n",
        "#tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "P-9M1z19gYnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating model with keras\n",
        "input1 = layers.Input(shape=(128,), dtype='int32')\n",
        "input2 = layers.Input(shape=(1899,), dtype='float32')\n",
        "attention_mask = layers.Input(shape=(128,), dtype='int32')\n",
        "\n",
        "\n",
        "##### Construct bert layer #####\n",
        "hidden_states = bert_model(input1, attention_mask = attention_mask)[1] # using pooler output\n",
        "\n",
        "cls = hidden_states\n",
        "cls= tf.concat(axis=1,values=[cls, input2])\n",
        "output = layers.Dense(77, activation=\"softmax\")(cls)\n",
        "\n",
        "cls_model = keras.Model(inputs= [input1, input2, attention_mask], outputs=output)\n",
        "\n",
        "\n",
        "# compile and fot the model\n",
        "cls_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
        ")\n",
        "\n",
        "\n",
        "s = EarlyStopping(monitor='val_loss', mode='min', verbose=5, patience = 3)\n",
        "\n",
        "history = cls_model.fit(\n",
        "    [train_input_ids, xtrain_tfv, train_mask],\n",
        "    ytrain,\n",
        "    validation_data=([valid_input_ids, xvalid_tfv, valid_mask], yvalid),\n",
        "    batch_size=32,\n",
        "    epochs=100,\n",
        "    callbacks = [es]\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u8dZPz4Sgbm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate the model on test dataset\n",
        "cls_model.evaluate([test_input_ids, xtest_tfv, test_mask], y_test, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "7_kUpL06gzf4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}